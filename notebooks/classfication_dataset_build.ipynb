{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d8f92795",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import torch\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms, models\n",
    "from PIL import Image\n",
    "import os\n",
    "import glob\n",
    "import tqdm\n",
    "from torchvision import transforms\n",
    "from zmq import device\n",
    "import numpy as np\n",
    "\n",
    "device = torch.device(\"cuda:2\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d9fff374",
   "metadata": {},
   "outputs": [],
   "source": [
    "if 0:\n",
    "# This script creates symbolic links in a destination directory for images from multiple source directories.\n",
    "    import os\n",
    "\n",
    "    dest_dir = \"/home/rishabh.mondal/Brick-Kilns-project/ijcai_2025_kilns/domain_experiment/data/continent_classification\"\n",
    "\n",
    "    src_dirs = [\n",
    "        \"/home/patel_zeel/kiln_compass_24/data/uttar_pradesh/images\",\n",
    "        \"/home/rishabh.mondal/Brick-Kilns-project/ijcai_2025_kilns/data/processed_data/pakistan/pak_punjab/images\",\n",
    "        \"/home/rishabh.mondal/Brick-Kilns-project/ijcai_2025_kilns/data/processed_data/bangladesh/dhaka/images\"\n",
    "    ]\n",
    "\n",
    "    for src in src_dirs:\n",
    "        folder_name = os.path.basename(os.path.dirname(src))  # e.g., uttar_pradesh, pak_punjab, dhaka\n",
    "        dest_subdir = os.path.join(dest_dir, folder_name)\n",
    "        os.makedirs(dest_subdir, exist_ok=True)\n",
    "\n",
    "        for file_name in os.listdir(src):\n",
    "            src_file = os.path.join(src, file_name)\n",
    "            dest_link = os.path.join(dest_subdir, file_name)\n",
    "\n",
    "            if not os.path.exists(dest_link):\n",
    "                os.symlink(src_file, dest_link)\n",
    "\n",
    "            # Crosscheck: verify link points to correct source\n",
    "            if os.path.islink(dest_link):\n",
    "                target = os.readlink(dest_link)\n",
    "                if os.path.abspath(target) != os.path.abspath(src_file):\n",
    "                    print(f\"Mismatch link: {dest_link} -> {target}\")\n",
    "            else:\n",
    "                print(f\"Missing symlink: {dest_link}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "761476b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "if 0:\n",
    "# This script splits images from multiple directories into train, val, and test sets.\n",
    "    import random\n",
    "    import shutil\n",
    "\n",
    "    source_dir = \"/home/rishabh.mondal/Brick-Kilns-project/ijcai_2025_kilns/domain_experiment/data/continent_classification\"\n",
    "    splits = ['train', 'val', 'test']\n",
    "    split_ratio = [0.60, 0.20, 0.20]  \n",
    "\n",
    "    for state in os.listdir(source_dir):\n",
    "        state_path = os.path.join(source_dir, state)\n",
    "        if not os.path.isdir(state_path):\n",
    "            continue\n",
    "\n",
    "        images = [f for f in os.listdir(state_path) if os.path.isfile(os.path.join(state_path, f))]\n",
    "        random.shuffle(images)\n",
    "        n = len(images)\n",
    "        train_end = int(n * split_ratio[0])\n",
    "        val_end = train_end + int(n * split_ratio[1])\n",
    "\n",
    "        split_data = {\n",
    "            'train': images[:train_end],\n",
    "            'val': images[train_end:val_end],\n",
    "            'test': images[val_end:]\n",
    "        }\n",
    "\n",
    "        for split in splits:\n",
    "            split_dir = os.path.join(state_path, split)\n",
    "            os.makedirs(split_dir, exist_ok=True)\n",
    "            for fname in split_data[split]:\n",
    "                src_file = os.path.join(state_path, fname)\n",
    "                dst_file = os.path.join(split_dir, fname)\n",
    "                if not os.path.exists(dst_file):\n",
    "                    os.symlink(src_file, dst_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6258fd26",
   "metadata": {},
   "outputs": [],
   "source": [
    "if 0:\n",
    "    print(\"Splitting completed. Check the directories for train, val, and test splits.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "af0c90c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "if 0:\n",
    "# This script creates a PyTorch dataset for image rotation tasks.\n",
    "    from torchvision import transforms\n",
    "    from torch.utils.data import Dataset\n",
    "    import random\n",
    "    from PIL import Image\n",
    "    import os\n",
    "\n",
    "    class RotationDataset(Dataset):\n",
    "        def __init__(self, image_paths, transform=None):\n",
    "            self.image_paths = image_paths\n",
    "            self.transform = transform\n",
    "            self.rotations = [0, 90, 180, 270]\n",
    "\n",
    "        def __len__(self):\n",
    "            return len(self.image_paths) * 4\n",
    "\n",
    "        def __getitem__(self, idx):\n",
    "            img_idx = idx // 4\n",
    "            rot_idx = idx % 4\n",
    "            angle = self.rotations[rot_idx]\n",
    "\n",
    "            image = Image.open(self.image_paths[img_idx]).convert('RGB')\n",
    "            image = image.rotate(angle)\n",
    "\n",
    "            if self.transform:\n",
    "                image = self.transform(image)\n",
    "\n",
    "            return image, rot_idx\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "43b751f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "if 1:\n",
    "# This script defines a PyTorch model for image rotation classification.\n",
    "    from torchvision import models\n",
    "    import torch.nn as nn\n",
    "\n",
    "    class RotationNet(nn.Module):\n",
    "        def __init__(self, base_model):\n",
    "            super().__init__()\n",
    "            self.backbone = nn.Sequential(*list(base_model.children())[:-1])  # remove FC\n",
    "            self.fc = nn.Linear(base_model.fc.in_features, 4)  # 4 rotation classes\n",
    "\n",
    "        def forward(self, x):\n",
    "            features = self.backbone(x)\n",
    "            features = features.view(features.size(0), -1)\n",
    "            out = self.fc(features)\n",
    "            return out, features  # return both logits and embedding\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4c2a467a",
   "metadata": {},
   "outputs": [],
   "source": [
    "if 0:\n",
    "# This script prepares the dataset and dataloader for training a rotation classification model.\n",
    "    images_dir = \"/home/rishabh.mondal/Brick-Kilns-project/ijcai_2025_kilns/domain_experiment/data/continent_classification\"\n",
    "    import glob\n",
    "    image_paths = glob.glob(os.path.join(images_dir,'uttar_pradesh','*.tif'))  # Adjust the pattern as needed\n",
    "    print(f\"Found {len(image_paths)} images.\")\n",
    "\n",
    "    dataset= RotationDataset(image_paths,\n",
    "                            transform=transforms.Compose([\n",
    "                                transforms.Resize((640, 640)),\n",
    "                                transforms.ToTensor(),\n",
    "                            \n",
    "                            ]))\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "78ec66dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "if 0:\n",
    "# This script trains a rotation classification model using the defined dataset and model.    \n",
    "    device = torch.device(\"cuda:1\" if torch.cuda.is_available() else \"cpu\")\n",
    "    dataloader = DataLoader(dataset, batch_size=64, shuffle=True, num_workers=8)\n",
    "\n",
    "    base_model = models.resnet50(pretrained=True)\n",
    "    model = RotationNet(base_model).to(device)\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "    num_epochs = 10\n",
    "\n",
    "    for epoch in tqdm.tqdm(range(num_epochs)):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        for i, (images, labels) in enumerate(dataloader):\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs, _ = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item() * images.size(0)\n",
    "            print(f\"Epoch [{epoch+1}/{num_epochs}], Iteration [{i+1}/{len(dataloader)}], Loss: {loss.item():.4f}\")\n",
    "\n",
    "        epoch_loss = running_loss / len(dataloader.dataset)\n",
    "        print(f\"Epoch [{epoch+1}/{num_epochs}] Completed, Avg Loss: {epoch_loss:.4f}\")\n",
    "        torch.save(model.state_dict(), f\"rotation_model_epoch_{epoch+1}.pth\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "536d773a",
   "metadata": {},
   "outputs": [],
   "source": [
    "if 0:\n",
    "# This script traces a PyTorch model for deployment using TorchScript.\n",
    "    import timm\n",
    "    import torch\n",
    "    model = timm.create_model('vit_base_patch16_224', img_size=640, pretrained=True)\n",
    "    dummy_input = torch.randn(1, 3, 640, 640)\n",
    "    traced_model = torch.jit.trace(model, dummy_input)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2f3915ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "if 1:\n",
    "\n",
    "    model_checkpoint=\"/home/rishabh.mondal/Brick-Kilns-project/ijcai_2025_kilns/domain_experiment/checkpoints/dhaka_resnet18_epoch_20.pth\"\n",
    "    base_model = models.resnet18(pretrained=True)\n",
    "\n",
    "    model= RotationNet(base_model)\n",
    "    model\n",
    "    # model.load_state_dict(torch.load(model_checkpoint, map_location='cpu'))\n",
    "    # model=model.to(device)\n",
    "    # model.eval()\n",
    "    # transform = transforms.Compose([\n",
    "    #     transforms.Resize((640, 640)),\n",
    "    #     transforms.ToTensor(),\n",
    "    #     transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "    # ])\n",
    "    # image_paths = glob.glob(\"/home/rishabh.mondal/Brick-Kilns-project/ijcai_2025_kilns/domain_experiment/data/continent_classification/dhaka/*.tif\")\n",
    "    # print(f\"Found {len(image_paths)} images.\")\n",
    "    # embeddings = []\n",
    "    # with torch.no_grad():\n",
    "    #     for img_path in tqdm.tqdm(image_paths):\n",
    "    #         img = Image.open(img_path).convert(\"RGB\")\n",
    "    #         img = transform(img).unsqueeze(0).to(device)\n",
    "    #         _, feat = model(img)\n",
    "    #         embeddings.append(feat.cpu().numpy())\n",
    "    # embeddings = np.concatenate(embeddings, axis=0)\n",
    "    # print(f\"Extracted {embeddings.shape[0]} embeddings of shape {embeddings.shape[1:]}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0f17952b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RotationNet(\n",
       "  (backbone): Sequential(\n",
       "    (0): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ReLU(inplace=True)\n",
       "    (3): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "    (4): Sequential(\n",
       "      (0): BasicBlock(\n",
       "        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (1): BasicBlock(\n",
       "        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (5): Sequential(\n",
       "      (0): BasicBlock(\n",
       "        (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "          (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): BasicBlock(\n",
       "        (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (6): Sequential(\n",
       "      (0): BasicBlock(\n",
       "        (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): BasicBlock(\n",
       "        (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (7): Sequential(\n",
       "      (0): BasicBlock(\n",
       "        (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): BasicBlock(\n",
       "        (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (8): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "  )\n",
       "  (fc): Linear(in_features=512, out_features=4, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f248795",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rishabh_sat",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
